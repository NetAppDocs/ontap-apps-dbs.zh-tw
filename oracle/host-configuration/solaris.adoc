---
sidebar: sidebar 
permalink: oracle/host-configuration/solaris.html 
keywords: oracle, database, ontap, solaris, zfs 
summary: 使用 Solaris 的 ONTAP 上的 Oracle 
---
= Solaris
:allow-uri-read: 


[role="lead"]
特定於 Solaris OS 的組態主題。



== Solaris NFS 掛載選項

下表列出單一執行個體的 Solaris NFS 掛載選項。

|===
| 檔案類型 | 掛載選項 


| ADR 首頁 | `rw,bg,hard,[vers=3,vers=4.1], roto=tcp, timeo=600,rsize=262144,wsize=262144` 


| 控制檔
資料檔案
重作記錄 | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp, timeo=600,rsize=262144,wsize=262144, nointr,llock,suid` 


| `ORACLE_HOME` | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp, timeo=600,rsize=262144,wsize=262144, suid` 
|===
的用途 `llock` 已獲證實、可移除在儲存系統上取得和釋放鎖定的相關延遲、大幅提升客戶環境的效能。在將多個伺服器設定為掛載相同檔案系統的環境中、請謹慎使用此選項、並將 Oracle 設定為掛載這些資料庫。雖然這是非常不尋常的組態、但只有少數客戶使用。如果第二次意外啟動某個執行個體、可能會因為 Oracle 無法偵測到外部伺服器上的鎖定檔案而導致資料毀損。NFS 鎖定不會提供保護；如同 NFS 第 3 版一樣、它們只是建議事項。

因為 `llock` 和 `forcedirectio` 參數是互斥的、這一點很重要 `filesystemio_options=setall` 存在於中 `init.ora` 檔案就是這樣 `directio` 已使用。如果沒有此參數、就會使用主機作業系統緩衝區快取、而且效能可能會受到負面影響。

下表列出了 Solaris NFS RAC 掛載選項。

|===
| 檔案類型 | 掛載選項 


| ADR 首頁 | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp,
timeo=600,rsize=262144,wsize=262144,
noac` 


| 控制檔
資料檔案
重作記錄 | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp,
timeo=600,rsize=262144,wsize=262144,
nointr,noac,forcedirectio` 


| CRS/ 投票 | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp,
timeo=600,rsize=262144,wsize=262144,
nointr,noac,forcedirectio` 


| 專屬 `ORACLE_HOME` | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp,
timeo=600,rsize=262144,wsize=262144,
suid` 


| 共享 `ORACLE_HOME` | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp,
timeo=600,rsize=262144,wsize=262144,
nointr,noac,suid` 
|===
單一執行個體與 RAC 掛載選項之間的主要差異在於新增 `noac` 和 `forcedirectio` 至掛載選項。此新增功能可停用主機作業系統快取、使 RAC 叢集中的所有執行個體都能一致地檢視資料狀態。雖然使用 `init.ora` 參數 `filesystemio_options=setall` 停用主機快取的效果相同、仍需使用 `noac` 和 `forcedirectio`。

原因 `actimeo=0` 為共享的必要項目 `ORACLE_HOME` 部署是為了促進檔案的一致性、例如 Oracle 密碼檔案和 spfiles 。如果 RAC 叢集中的每個執行個體都有專用的 `ORACLE_HOME`，不需要此參數。



== Solaris UFS 掛載選項

NetApp 強烈建議您使用記錄掛載選項、以便在 Solaris 主機當機或 FC 連線中斷時保留資料完整性。記錄掛載選項也可保留 Snapshot 備份的使用性。



== Solaris ZFS

必須仔細安裝和設定 Solaris ZFS 、才能提供最佳效能。



=== mvector

Solaris 11 變更了 IT 處理大型 I/O 作業的方式、可能會在 SAN 儲存陣列上造成嚴重的效能問題。NetApp 錯誤報告 630173 「 Solaris 11 ZFS 效能回歸」中詳細說明了此問題。" 解決方案是變更名為的 OS 參數 `zfs_mvector_max_size`。

以 root 執行下列命令：

....
[root@host1 ~]# echo "zfs_mvector_max_size/W 0t131072" |mdb -kw
....
如果這項變更發生任何非預期的問題、您可以以 root 執行下列命令、輕鬆地將其還原：

....
[root@host1 ~]# echo "zfs_mvector_max_size/W 0t1048576" |mdb -kw
....


== 核心

可靠的 ZFS 效能需要修補 Solaris 核心、以因應 LUN 對齊問題。此修正程式是隨 Solaris 10 中的修補程式 147440-19 和適用於 Solaris 11 的 SRU 10.5 一起推出的。只能將 Solaris 10 及更新版本與 ZFS 搭配使用。



== LUN 組態

若要設定 LUN 、請完成下列步驟：

. 建立類型的 LUN `solaris`。
. 安裝所指定的適當主機公用程式套件（ Huk ） link:https://imt.netapp.com/matrix/#search["NetApp互通性對照表工具IMT （不含）"^]。
. 請依照 Huk 中的說明進行操作、完全符合上述說明。以下概述基本步驟、但請參閱 link:https://docs.netapp.com/us-en/ontap-sanhost/index.html["最新文件"^] 以瞭解正確的程序。
+
.. 執行 `host_config` 更新的公用程式 `sd.conf/sdd.conf` 檔案：這樣做可讓 SCSI 磁碟機正確探索 ONTAP LUN 。
.. 請遵循所提供的指示 `host_config` 啟用多重路徑輸入 / 輸出（ MPIO ）的公用程式。
.. 重新開機。此步驟是必要步驟、以便在整個系統中辨識任何變更。


. 分割 LUN 並確認它們已正確對齊。請參閱「附錄 B ： WAFL 校準驗證」、瞭解如何直接測試及確認校準。




=== zPools

只有在中的步驟之後才應建立 zpool link:solaris.html#lun-configuration["LUN 組態"] 執行。如果程序未正確執行、可能會因為 I/O 對齊而導致嚴重的效能降低。ONTAP 的最佳效能要求 I/O 必須與磁碟機上的 4K 邊界對齊。在 zpool 上建立的檔案系統使用有效的區塊大小、並透過稱為的參數加以控制 `ashift`，您可以執行命令來檢視 `zdb -C`。

的價值 `ashift` 預設為 9 、表示 2^9 或 512 位元組。為了獲得最佳效能 `ashift` 值必須為 12 （ 2^12=4K ）。此值是在創建 zpool 時設置的，不能更改，這意味着 zpool 中的數據 `ashift` 除 12 個以外、應將資料複製到新建立的 zPool 、以進行移轉。

建立 zPool 之後、請驗證的值 `ashift` 繼續之前。如果值不是 12 、則表示未正確探索到 LUN 。銷毀 zpool 、確認相關主機公用程式文件中顯示的所有步驟均已正確執行、然後重新建立 zPool 。



=== zPools 和 Solaris LDoms

Solaris LDoms 還需要確保 I/O 對齊正確無誤。雖然 LUN 可能會被正確發現為 4K 裝置、但 LDOM 上的虛擬 vdsk 裝置不會繼承 I/O 網域的組態。以該 LUN 為基礎的 vdsk 預設為 512 位元組區塊。

需要額外的組態檔案。首先、必須針對 Oracle 錯誤 15824910 修補個別的 LDOM 、才能啟用其他組態選項。此修補程式已移轉至所有目前使用的 Solaris 版本。一旦 LDOM 獲得修補、就可以依照下列方式設定新的正確對齊 LUN ：

. 識別要在新的 zPool 中使用的 LUN 或 LUN 。在此範例中、它是 c2d1 裝置。
+
....
[root@LDOM1 ~]# echo | format
Searching for disks...done
AVAILABLE DISK SELECTIONS:
  0. c2d0 <Unknown-Unknown-0001-100.00GB>
     /virtual-devices@100/channel-devices@200/disk@0
  1. c2d1 <SUN-ZFS Storage 7330-1.0 cyl 1623 alt 2 hd 254 sec 254>
     /virtual-devices@100/channel-devices@200/disk@1
....
. 擷取要用於 ZFS Pool 的裝置之 VDC 執行個體：
+
....
[root@LDOM1 ~]#  cat /etc/path_to_inst
#
# Caution! This file contains critical kernel state
#
"/fcoe" 0 "fcoe"
"/iscsi" 0 "iscsi"
"/pseudo" 0 "pseudo"
"/scsi_vhci" 0 "scsi_vhci"
"/options" 0 "options"
"/virtual-devices@100" 0 "vnex"
"/virtual-devices@100/channel-devices@200" 0 "cnex"
"/virtual-devices@100/channel-devices@200/disk@0" 0 "vdc"
"/virtual-devices@100/channel-devices@200/pciv-communication@0" 0 "vpci"
"/virtual-devices@100/channel-devices@200/network@0" 0 "vnet"
"/virtual-devices@100/channel-devices@200/network@1" 1 "vnet"
"/virtual-devices@100/channel-devices@200/network@2" 2 "vnet"
"/virtual-devices@100/channel-devices@200/network@3" 3 "vnet"
"/virtual-devices@100/channel-devices@200/disk@1" 1 "vdc" << We want this one
....
. 編輯 `/platform/sun4v/kernel/drv/vdc.conf`：
+
....
block-size-list="1:4096";
....
+
這表示裝置執行個體 1 的區塊大小為 4096 。

+
另一個範例是假設需要將 vdsk 執行個體 1 至 6 設定為 4K 區塊大小和 `/etc/path_to_inst` 內容如下：

+
....
"/virtual-devices@100/channel-devices@200/disk@1" 1 "vdc"
"/virtual-devices@100/channel-devices@200/disk@2" 2 "vdc"
"/virtual-devices@100/channel-devices@200/disk@3" 3 "vdc"
"/virtual-devices@100/channel-devices@200/disk@4" 4 "vdc"
"/virtual-devices@100/channel-devices@200/disk@5" 5 "vdc"
"/virtual-devices@100/channel-devices@200/disk@6" 6 "vdc"
....
. 最終結果 `vdc.conf` 檔案應包含下列項目：
+
....
block-size-list="1:8192","2:8192","3:8192","4:8192","5:8192","6:8192";
....
+
|===
| 注意 


| 設定 VC.conf 並建立 vdsk 之後、必須重新啟動 LDOM 。無法避免此步驟。區塊大小變更只會在重新開機後生效。繼續使用 zpool 組態、並確保如前所述、移位已正確設定為 12 。 
|===




=== ZFS Intent Log （ ZIL ）

一般而言、沒有理由在不同的裝置上找到 ZFS Intent Log （ ZIL ）。記錄檔可以與主集區共用空間。獨立 ZIL 的主要用途是使用缺乏現代儲存陣列寫入快取功能的實體磁碟機。



=== logbias

設定 `logbias` 託管 Oracle 資料的 ZFS 檔案系統參數。

....
zfs set logbias=throughput <filesystem>
....
使用此參數可降低整體寫入層級。根據預設值、寫入的資料會先提交至 ZIL 、然後再提交至主儲存池。此方法適用於使用純磁碟機組態的組態、包括 SSD 型 ZIL 裝置和主儲存池的旋轉媒體。這是因為它允許在可用的最低延遲媒體上、在單一 I/O 交易中進行認可。

使用包含其快取功能的現代化儲存陣列時、通常不需要使用此方法。在極少數情況下、可能需要在單一交易中寫入記錄檔、例如由高度集中、對延遲敏感的隨機寫入所組成的工作負載。寫入放大的形式會產生影響、因為記錄的資料最終會寫入主儲存池、導致寫入活動加倍。



=== 直接 I/O

許多應用程式（包括 Oracle 產品）都可以啟用直接 I/O 、藉此略過主機緩衝區快取此策略無法在 ZFS 檔案系統中正常運作。雖然會略過主機緩衝區快取、但 ZFS 本身仍會繼續快取資料。使用 Fio 或 Sio 等工具執行效能測試時、這項動作可能會產生誤導性的結果、因為很難預測 I/O 是否到達儲存系統、或是是否在作業系統中本機快取。此動作也會讓使用此類模擬測試來比較 ZFS 效能與其他檔案系統的情況變得非常困難。實際上、在真實使用者工作負載下、檔案系統效能幾乎沒有任何差異。



=== 多個 zPools

必須在 zpool 層級執行快照型備份、還原、複製及歸檔 ZFS 型資料、而且通常需要多個 zPools 。zpool 類似於 LVM 磁碟群組、應使用相同的規則進行設定。例如、資料庫的配置最好是存放在資料檔案上 `zpool1` 以及駐留在上的歸檔記錄、控制檔和重做記錄 `zpool2`。此方法允許標準熱備份、將資料庫置於熱備份模式、然後是的快照 `zpool1`。接著會從熱備份模式移除資料庫、強制進行記錄歸檔、並建立快照 `zpool2` 已建立。還原作業需要卸載 zfs 檔案系統、並在執行 SnapRestore 還原作業之後、將 zPool 完全離線。然後可以重新上線並恢復資料庫。



=== filesystemio_options

Oracle 參數 `filesystemio_options` 使用 ZFS 的方式不同。如果 `setall` 或 `directio` 使用時、寫入作業會同步並略過 OS 緩衝區快取、但讀取會由 ZFS 進行緩衝。此動作會導致效能分析方面的困難、因為有時會被 ZFS 快取攔截和服務 I/O 、使儲存延遲和總 I/O 比預期的要少。
