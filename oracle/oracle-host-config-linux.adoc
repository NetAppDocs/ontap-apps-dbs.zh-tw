---
sidebar: sidebar 
permalink: oracle/oracle-host-config-linux.html 
keywords: oracle, database, ontap, linux, nfs, xfs, ext4, slot tables 
summary: 使用 Linux 的 Oracle 資料庫 
searchtitle: 使用 Linux 的 Oracle 資料庫 
---
= Linux
:allow-uri-read: 


[role="lead"]
Linux 作業系統專屬的組態主題。



== Linux NFSv3 TCP 插槽表

TCP 插槽表是與主機匯流排介面卡（ HBA ）佇列深度相當的 NFSv3 。這些表格可控制任何時間都可以處理的NFS作業數量。預設值通常為16、這對於最佳效能而言太低。相反的問題發生在較新的Linux核心上、這會自動將TCP插槽表格限制增加到要求使NFS伺服器飽和的層級。

為了達到最佳效能並避免效能問題、請調整控制 TCP 插槽表的核心參數。

執行 `sysctl -a | grep tcp.*.slot_table` 並觀察下列參數：

....
# sysctl -a | grep tcp.*.slot_table
sunrpc.tcp_max_slot_table_entries = 128
sunrpc.tcp_slot_table_entries = 128
....
所有 Linux 系統都應該包括在內 `sunrpc.tcp_slot_table_entries`但只有部分包含在內 `sunrpc.tcp_max_slot_table_entries`。兩者都應設為 128 。

|===
| 注意 


| 若未設定這些參數、可能會對效能造成重大影響。在某些情況下、效能會受到限制、因為 Linux 作業系統沒有發出足夠的 I/O在其他情況下、隨著 Linux 作業系統嘗試發出的 I/O 數量超過可服務的數量、 I/O 延遲也會增加。 
|===


== Linux NFS 裝載選項

下表列出單一執行個體的 Linux NFS 掛載選項。

|===
| 檔案類型 | 掛載選項 


| ADR 首頁 | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp,
timeo=600,rsize=262144,wsize=262144` 


| 控制檔
資料檔案
重作記錄 | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp,
timeo=600,rsize=262144,wsize=262144,
nointr` 


| Oracle_Home | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp,
timeo=600,rsize=262144,wsize=262144,
nointr` 
|===
下表列出 RAC 的 Linux NFS 掛載選項。

|===
| 檔案類型 | 掛載選項 


| ADR 首頁 | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp,
timeo=600,rsize=262144,wsize=262144,
actimeo=0` 


| 控制檔
資料檔案
重作記錄 | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp,
timeo=600,rsize=262144,wsize=262144,
nointr,actimeo=0` 


| CRS/ 投票 | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp,
timeo=600,rsize=262144,wsize=262144,
nointr,noac,actimeo=0` 


| 專屬 `ORACLE_HOME` | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp,
timeo=600,rsize=262144,wsize=262144` 


| 共享 `ORACLE_HOME` | `rw,bg,hard,[vers=3,vers=4.1],proto=tcp,
timeo=600,rsize=262144,wsize=262144,
nointr,actimeo=0` 
|===
單一執行個體與 RAC 掛載選項之間的主要差異在於新增 `actimeo=0` 至掛載選項。此新增功能可停用主機作業系統快取、使 RAC 叢集中的所有執行個體都能一致地檢視資料狀態。雖然使用 `init.ora` 參數 `filesystemio_options=setall` 停用主機快取的效果相同、仍需使用 `actimeo=0`。

原因 `actimeo=0` 為共享的必要項目 `ORACLE_HOME` 部署是為了促進檔案的一致性、例如 Oracle 密碼檔案和 spfiles 。如果 RAC 叢集中的每個執行個體都有專用的 `ORACLE_HOME`，則不需要此參數。

一般而言、非資料庫檔案的掛載應該與單一執行個體資料檔案所使用的選項相同、不過特定應用程式可能有不同的需求。避免使用掛載選項 `noac` 和 `actimeo=0` 如果可能、因為這些選項會停用檔案系統層級的預先讀取和緩衝處理。這可能會對擷取、轉譯和載入等程序造成嚴重的效能問題。



=== 存取與 GetAttr

有些客戶指出、存取和 GetAttr 等極高層級的其他 IOPS 、可能會主導他們的工作負載。在極端情況下、讀取和寫入等作業可低於總作業量的 10% 。這是任何包含使用的資料庫的正常行為 `actimeo=0` 和/或 `noac` 在 Linux 上、因為這些選項會導致 Linux 作業系統持續從儲存系統重新載入檔案中繼資料。存取和 GetAttr 等作業是從資料庫環境中的 ONTAP 快取提供服務的低影響作業。它們不應被視為真正的 IOPS 、例如讀寫、而會對儲存系統產生真正的需求。不過、這些其他 IOPS 確實會產生一些負載、尤其是在 RAC 環境中。若要解決這種情況、請啟用 DNFS 、以略過作業系統緩衝區快取、避免這些不必要的中繼資料作業。



=== Linux Direct NFS

另一個掛載選項、稱為 `nosharecache`（ a ） DNFS 啟用時、需要使用、（ b ）來源磁碟區多次裝載於具有巢狀 NFS 裝載的單一伺服器（ c ）上。此組態主要出現在支援 SAP 應用程式的環境中。例如、 NetApp 系統上的單一磁碟區可能會有位於的目錄 `/vol/oracle/base` 再來一次 `/vol/oracle/home`。如果 `/vol/oracle/base` 安裝於 `/oracle` 和 `/vol/oracle/home` 安裝於 `/oracle/home`，結果是來自相同來源的巢狀 NFS 掛載。

作業系統可以偵測並 `/oracle/home`位於同一個磁碟區（即相同的來源檔案系統）上的事實 `/oracle`。作業系統接著會使用相同的裝置控制代碼來存取資料。這樣做可以改善 OS 快取和某些其他作業的使用、但會干擾 DNFS 。如果 DNFS 必須存取上的 `/oracle/home`檔案 `spfile`，可能會錯誤地嘗試使用錯誤的資料路徑。結果是 I/O 作業失敗。在這些組態中、將裝載選項新增 `nosharecache`至任何與該主機上其他 NFS 檔案系統共用來源 Volume 的 NFS 檔案系統。這樣做會強制 Linux 作業系統為該檔案系統分配獨立的裝置控制代碼。



=== Linux Direct NFS 和 Oracle RAC

使用 DNFS 對 Linux 作業系統上的 Oracle RAC 有特殊的效能優勢、因為 Linux 沒有強制直接 I/O 的方法、而 RAC 需要此方法才能在節點之間保持一致。因應措施是 Linux 需要使用 `actimeo=0` 掛載選項、會使檔案資料從作業系統快取立即過期。此選項會強制 Linux NFS 用戶端持續重新讀取屬性資料、進而損害延遲並增加儲存控制器的負載。

啟用 DNFS 會略過主機 NFS 用戶端、避免此損害。多家客戶在啟用 DNFS 時、報告 RAC 叢集的效能大幅提升、 ONTAP 負載大幅降低（特別是其他 IOPS ）。



=== Linux Direct NFS 和 oranfstab 檔案

在 Linux 上搭配多重路徑選項使用 DNFS 時、必須使用多個子網路。在其他作業系統上、可使用建立多個 DNFS 通道 `LOCAL` 和 `DONTROUTE` 可在單一子網路上設定多個 DNFS 通道的選項。不過、這在 Linux 上無法正常運作、因此可能會產生非預期的效能問題。在 Linux 中、用於 DNFS 流量的每個 NIC 都必須位於不同的子網路上。



=== I/O 排程器

Linux 核心可讓您以低層級控制 I/O 排程封鎖裝置的方式。Linux 各版本的預設值差異極大。測試顯示、截止日期通常會提供最佳結果、但有時 NOOP 會稍微好一點。效能差異最小、但如果需要從資料庫組態擷取最大可能效能、請測試這兩個選項。在許多組態中、 CFQ 是預設值、而且已證明資料庫工作負載的效能有重大問題。

如需設定 I/O 排程器的指示、請參閱相關的 Linux 廠商文件。



=== 多重路徑

部分客戶在網路中斷期間遭遇當機、因為多重路徑常駐程式未在其系統上執行。在最新版本的 Linux 上、作業系統的安裝程序和多重路徑常駐程式可能會讓這些作業系統容易受到此問題的影響。套件已正確安裝、但未設定為在重新開機後自動啟動。

例如、 RHEL5.5 上多重路徑常駐程式的預設值可能如下所示：

....
[root@host1 iscsi]# chkconfig --list | grep multipath
multipathd      0:off   1:off   2:off   3:off   4:off   5:off   6:off
....
您可以使用下列命令來修正此問題：

....
[root@host1 iscsi]# chkconfig multipathd on
[root@host1 iscsi]# chkconfig --list | grep multipath
multipathd      0:off   1:off   2:on    3:on    4:on    5:on    6:off
....


== ASM 鏡像

ASM鏡射可能需要變更Linux多重路徑設定、以允許ASM辨識問題並切換至其他故障群組。大部分關於「不完整」的ASM組態ONTAP 都使用外部備援、這表示資料保護是由外部陣列提供、而ASM不會鏡射資料。某些站台使用具有一般備援的ASM來提供雙向鏡像、通常是跨不同站台。

中顯示的 Linux 設定 link:https://docs.netapp.com/us-en/ontap-sanhost/hu_fcp_scsi_index.html["NetApp 主機公用程式文件"] 包含會導致 I/O 無限期佇列的多重路徑參數這表示 LUN 裝置上沒有作用中路徑的 I/O 會在 I/O 完成所需的時間內等待。這通常是很理想的做法、因為 Linux 主機會在 SAN 路徑變更完成、 FC 交換器重新開機或儲存系統完成容錯移轉所需的時間內等待。

這種不受限制的佇列行為會導致 ASM 鏡像發生問題、因為 ASM 必須收到 I/O 故障、才能在替代 LUN 上重試 I/O 。

在 Linux 中設定下列參數 `multipath.conf` 用於 ASM 鏡像的 ASM LUN 檔案：

....
polling_interval 5
no_path_retry 24
....
這些設定會為 ASM 裝置建立 120 秒的逾時。逾時會計算為 `polling_interval` * `no_path_retry` 秒。在某些情況下可能需要調整確切的值、但 120 秒的逾時時間應足以滿足大部分的使用需求。具體而言、 120 秒的時間應該能讓控制器接管或恢復、而不會產生 I/O 錯誤、導致故障群組離線。

較低 `no_path_retry` 此值可縮短 ASM 切換至替代故障群組所需的時間、但這也會增加在維護活動（例如控制器接管）期間不必要的容錯移轉風險。仔細監控 ASM 鏡像狀態、即可降低風險。如果發生不必要的容錯移轉、只要執行重新同步的速度相對較快、鏡像就能快速重新同步。如需更多資訊、請參閱 ASM Fast Mirror Resync 上的 Oracle 說明文件、以瞭解所使用的 Oracle 軟體版本。



== Linux xfs 、 ext3 和 ext4 掛載選項


TIP: * NetApp 建議 * 使用預設掛載選項。
